{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First approach: Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/train_imdb_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews: 41669\n",
      "Positive reviews: 27295\n",
      "Negative reviews: 14374\n",
      "ratio: 1.8989147071100598\n",
      "-------------------------\n",
      "Positive reviews: 14374\n",
      "Negative reviews: 14374\n",
      "ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "# This cell is only for experiments of balancing dataset sentiments\n",
    "# We don't use this cell for the original experiments, only for the experiments of balancing dataset sentiments\n",
    "\n",
    "positive_count = df[df[\"sentiment\"] == 1].shape[0]\n",
    "negative_count = df[df[\"sentiment\"] == 0].shape[0]\n",
    "print(\"Total reviews:\", df.shape[0])\n",
    "print(\"Positive reviews:\", positive_count)\n",
    "print(\"Negative reviews:\", negative_count)\n",
    "print(\"ratio:\", positive_count/negative_count)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "\n",
    "positive = df[df[\"sentiment\"] == 1][:negative_count]\n",
    "negative = df[df[\"sentiment\"] == 0]\n",
    "print(\"Positive reviews:\", positive.shape[0])\n",
    "print(\"Negative reviews:\", negative.shape[0])\n",
    "print(\"ratio:\", positive.shape[0]/negative.shape[0])\n",
    "\n",
    "df = pd.concat([positive, negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training data, and convert to numpy array\n",
    "X_train = df['review']\n",
    "y_train = df['sentiment']\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from itertools import chain\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# get the frequency distribution of the words\n",
    "freqDist = FreqDist()\n",
    "for review in X_train:\n",
    "    # tokenize the review, and add the frequency of each word to the frequency distribution\n",
    "    tokens = word_tokenize(review)\n",
    "    for word in tokens:\n",
    "        freqDist[word.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only use the 10000 most frequent words as features\n",
    "word_features = list(freqDist.keys())[0:10000]\n",
    "\n",
    "# define a function to extract the features of a document\n",
    "def document_features(document):\n",
    "    # convert the document to a set of unique words\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        # create a feature for each word, indicating whether the document contains that word\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the tokenized features with the labels\n",
    "combined_data_train = [(document_features(word_tokenize(review)), sentiment) for review, sentiment in zip(X_train, y_train)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a Naive Bayes classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do 5-fold cross-validation to evaluate the classifier\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "# set the number of folds and the size of each fold\n",
    "num_folds = 5\n",
    "subset_size = len(combined_data_train) // num_folds\n",
    "# initialize the best classifier and the best accuracy, since we want to get the best classifier in the end\n",
    "best_classifier = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# to store the confusion matrix, we need to accumulate the values for each fold\n",
    "train_tp = 0\n",
    "train_fp = 0\n",
    "train_fn = 0\n",
    "train_tn = 0\n",
    "\n",
    "for i in range(num_folds):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "    # get the testing and training data for this fold\n",
    "    testing_this_round = combined_data_train[i*subset_size:][:subset_size]\n",
    "    training_this_round = combined_data_train[:i*subset_size] + combined_data_train[(i+1)*subset_size:]\n",
    "\n",
    "    # train the classifier for this round\n",
    "    classifier = NaiveBayesClassifier.train(training_this_round)\n",
    "    \n",
    "    # test the classifier for this round\n",
    "    predictions = []\n",
    "    for review, sentiment in testing_this_round:\n",
    "        predictions.append(classifier.classify(review))\n",
    "    \n",
    "    # calculate the confusion matrix for this round\n",
    "    for j in range(len(predictions)):\n",
    "        if predictions[j] == 1:\n",
    "            if testing_this_round[j][1] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if testing_this_round[j][1] == 1:\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "    # accumulate to the confusion matrix values\n",
    "    train_tp += tp\n",
    "    train_fp += fp\n",
    "    train_fn += fn\n",
    "    train_tn += tn\n",
    "\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    print('Round', i, 'accuracy:', acc)\n",
    "    # if the classifier for this round is the best so far, save it\n",
    "    if acc > best_accuracy:\n",
    "        print('currently best classifier for round', i)\n",
    "        best_accuracy = acc\n",
    "        best_classifier = classifier\n",
    "\n",
    "print(\"true positive:\", train_tp)\n",
    "print(\"true negative:\", train_tn)\n",
    "print(\"false positive:\", train_fp)\n",
    "print(\"false negative:\", train_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the evaluation metrics for training process\n",
    "def auc(tp, fp, tn, fn):\n",
    "    return (tp / (tp + fn) + tn / (tn + fp)) / 2\n",
    "\n",
    "tp = train_tp\n",
    "fp = train_fp\n",
    "tn = train_tn\n",
    "fn = train_fn\n",
    "\n",
    "print(\"true positives: \", tp)\n",
    "print(\"true negatives: \", tn)\n",
    "print(\"false positives: \", fp)\n",
    "print(\"false negatives: \", fn)\n",
    "\n",
    "acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1: \", f1)\n",
    "print(\"AUC: \", auc(tp, fp, tn, fn))\n",
    "print(tp + tn + fp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the final classifier again with all training dataset using the best classifier from the cross-validation\n",
    "final_classifier = best_classifier.train(combined_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final classifier\n",
    "import pickle\n",
    "\n",
    "with open('nbc_balanced_f_{}.pkl'.format(round(acc, 4)), 'wb') as f:\n",
    "    pickle.dump(final_classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4630"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the testing dataset\n",
    "df_test = pd.read_csv('../data/test_imdb_reviews.csv')\n",
    "\n",
    "X_test = df_test['review'].astype(str)\n",
    "y_test = df_test['sentiment']\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "len(X_test)\n",
    "len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same tokenization and feature extraction for the testing dataset\n",
    "combined_data_test = [(document_features(word_tokenize(review)), sentiment) for review, sentiment in zip(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "# test the final classifier with the testing dataset\n",
    "for review, sentiment in combined_data_test:\n",
    "    predictions.append(final_classifier.classify(review))\n",
    "\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the confusion matrix for the testing dataset\n",
    "test_tp = 0\n",
    "test_tn = 0\n",
    "test_fp = 0\n",
    "test_fn = 0\n",
    "for i, prediction in enumerate(predictions):\n",
    "    if prediction == y_test[i]:\n",
    "        if prediction == 1:\n",
    "            test_tp += 1\n",
    "        else:\n",
    "            test_tn += 1\n",
    "    else:\n",
    "        if prediction == 1:\n",
    "            test_fp += 1\n",
    "        else:\n",
    "            test_fn += 1\n",
    "\n",
    "print(\"true positives: \", test_tp)\n",
    "print(\"true negatives: \", test_tn)\n",
    "print(\"false positives: \", test_fp)\n",
    "print(\"false negatives: \", test_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the evaluation metrics for testing process\n",
    "\n",
    "acc = (test_tp + test_tn) / (test_tp + test_tn + test_fp + test_fn)\n",
    "precision = test_tp / (test_tp + test_fp)\n",
    "recall = test_tp / (test_tp + test_fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1: \", f1)\n",
    "print(\"AUROC: \", auc(test_tp, test_fp, test_tn, test_fn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
