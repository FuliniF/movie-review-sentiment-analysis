{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second approach: Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/train_imdb_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews: 41669\n",
      "Positive reviews: 27295\n",
      "Negative reviews: 14374\n",
      "ratio: 1.8989147071100598\n",
      "-------------------------\n",
      "Positive reviews: 14374\n",
      "Negative reviews: 14374\n",
      "ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "# This cell is only for experiments of balancing dataset sentiments\n",
    "# We don't use this cell for the original experiments, only for the experiments of balancing dataset sentiments\n",
    "\n",
    "positive_count = df[df[\"sentiment\"] == 1].shape[0]\n",
    "negative_count = df[df[\"sentiment\"] == 0].shape[0]\n",
    "print(\"Total reviews:\", df.shape[0])\n",
    "print(\"Positive reviews:\", positive_count)\n",
    "print(\"Negative reviews:\", negative_count)\n",
    "print(\"ratio:\", positive_count/negative_count)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "\n",
    "positive = df[df[\"sentiment\"] == 1][:negative_count]\n",
    "negative = df[df[\"sentiment\"] == 0]\n",
    "print(\"Positive reviews:\", positive.shape[0])\n",
    "print(\"Negative reviews:\", negative.shape[0])\n",
    "print(\"ratio:\", positive.shape[0]/negative.shape[0])\n",
    "\n",
    "df = pd.concat([positive, negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset we use in training, and convert them to numpy arrays\n",
    "X_train = df['review']\n",
    "y_train = df['sentiment']\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a Tfidfvectorizer and fit it to the training set\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# define a scorer that returns the numbers in a confusion matrix, and accuracy\n",
    "def confusion_matrix_scorer(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp, 'accuracy': acc}\n",
    "\n",
    "# create a model\n",
    "model = SVC(kernel='linear', C=1)\n",
    "# use 5-fold cross validation to evaluate the model\n",
    "scores = cross_validate(model, X_train, y_train, cv=5, \n",
    "                        scoring=confusion_matrix_scorer, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially set the best estimator to the first estimator, and the best score to the first score\n",
    "best_estimator = scores['estimator'][0]\n",
    "best_score = scores['test_accuracy'][0]\n",
    "b = 0\n",
    "\n",
    "# we want to find the best estimator first and train the best estimator on all train set again\n",
    "for i, estimator in enumerate(scores['estimator']):\n",
    "    if scores['test_accuracy'][i] > best_score:\n",
    "        best_estimator = estimator\n",
    "        best_score = scores['test_accuracy'][i]\n",
    "        b = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fit_time', 'score_time', 'estimator', 'test_tn', 'test_fp', 'test_fn', 'test_tp', 'test_accuracy'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell prints the confusion matrix of the best estimator on training dataset\n",
    "train_tp = scores['test_tp'][b]\n",
    "train_fp = scores['test_fp'][b]\n",
    "train_tn = scores['test_tn'][b]\n",
    "train_fn = scores['test_fn'][b]\n",
    "\n",
    "print('true positive:', train_tp)\n",
    "print('false positive:', train_fp)\n",
    "print('true negative:', train_tn)\n",
    "print('false negative:', train_fn)\n",
    "\n",
    "# compute all metrics as below\n",
    "def auc(tp, fp, tn, fn):\n",
    "    return (tp / (tp + fn) + tn / (tn + fp)) / 2\n",
    "\n",
    "acc = (train_tp + train_tn) / (train_tp + train_tn + train_fp + train_fn)\n",
    "precision = train_tp / (train_tp + train_fp)\n",
    "recall = train_tp / (train_tp + train_fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1: \", f1)\n",
    "print(\"AUC: \", auc(train_tp, train_fp, train_tn, train_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the best estimator again on the whole training set\n",
    "# to use it for prediction on testing dataset\n",
    "best_estimator = best_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the testing dataset\n",
    "\n",
    "df = pd.read_csv('../data/test_imdb_reviews.csv')\n",
    "X_test = df['review']\n",
    "y_test = df['sentiment']\n",
    "\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the testing dataset\n",
    "y_pred = best_estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confusion matrix of the prediction\n",
    "\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == y_pred[i]:\n",
    "        if y_test[i] == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    else:\n",
    "        if y_test[i] == 1:\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "print('True Positives:', tp)\n",
    "print('True Negatives:', tn)\n",
    "print('False Positives:', fp)\n",
    "print('False Negatives:', fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all metrics for testing dataset\n",
    "\n",
    "def auc(tp, fp, tn, fn):\n",
    "    return (tp / (tp + fn) + tn / (tn + fp)) / 2\n",
    "\n",
    "acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1: \", f1)\n",
    "print(\"AUC: \", auc(tp, fp, tn, fn))\n",
    "print(tp + tn + fp + fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
