{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second approach: Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/train_imdb_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews: 41669\n",
      "Positive reviews: 27295\n",
      "Negative reviews: 14374\n",
      "ratio: 1.8989147071100598\n",
      "-------------------------\n",
      "Positive reviews: 14374\n",
      "Negative reviews: 14374\n",
      "ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "# This cell is only for experiments of balancing dataset sentiments\n",
    "\n",
    "positive_count = df[df[\"sentiment\"] == 1].shape[0]\n",
    "negative_count = df[df[\"sentiment\"] == 0].shape[0]\n",
    "print(\"Total reviews:\", df.shape[0])\n",
    "print(\"Positive reviews:\", positive_count)\n",
    "print(\"Negative reviews:\", negative_count)\n",
    "print(\"ratio:\", positive_count/negative_count)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "\n",
    "positive = df[df[\"sentiment\"] == 1][:negative_count]\n",
    "negative = df[df[\"sentiment\"] == 0]\n",
    "print(\"Positive reviews:\", positive.shape[0])\n",
    "print(\"Negative reviews:\", negative.shape[0])\n",
    "print(\"ratio:\", positive.shape[0]/negative.shape[0])\n",
    "\n",
    "df = pd.concat([positive, negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28748\n",
      "28748\n"
     ]
    }
   ],
   "source": [
    "X_train = df['review']\n",
    "y_train = df['sentiment']\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def confusion_matrix_scorer(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp, 'accuracy': acc}\n",
    "\n",
    "model = SVC(kernel='linear', C=1)\n",
    "scores = cross_validate(model, X_train, y_train, cv=5, scoring=confusion_matrix_scorer, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = scores['estimator'][0]\n",
    "best_score = scores['test_accuracy'][0]\n",
    "b = 0\n",
    "\n",
    "for i, estimator in enumerate(scores['estimator']):\n",
    "    if scores['test_accuracy'][i] > best_score:\n",
    "        best_estimator = estimator\n",
    "        best_score = scores['test_accuracy'][i]\n",
    "        b = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fit_time', 'score_time', 'estimator', 'test_tn', 'test_fp', 'test_fn', 'test_tp', 'test_accuracy'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positive: 2473\n",
      "false positive: 359\n",
      "true negative: 2515\n",
      "false negative: 402\n",
      "Accuracy:  0.8676291528961558\n",
      "Precision:  0.8732344632768362\n",
      "Recall:  0.8601739130434782\n",
      "F1:  0.8666549851060102\n",
      "AUC:  0.867630449910744\n"
     ]
    }
   ],
   "source": [
    "train_tp = scores['test_tp'][b]\n",
    "train_fp = scores['test_fp'][b]\n",
    "train_tn = scores['test_tn'][b]\n",
    "train_fn = scores['test_fn'][b]\n",
    "\n",
    "print('true positive:', train_tp)\n",
    "print('false positive:', train_fp)\n",
    "print('true negative:', train_tn)\n",
    "print('false negative:', train_fn)\n",
    "\n",
    "def auc(tp, fp, tn, fn):\n",
    "    return (tp / (tp + fn) + tn / (tn + fp)) / 2\n",
    "\n",
    "acc = (train_tp + train_tn) / (train_tp + train_tn + train_fp + train_fn)\n",
    "precision = train_tp / (train_tp + train_fp)\n",
    "recall = train_tp / (train_tp + train_fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1: \", f1)\n",
    "print(\"AUC: \", auc(train_tp, train_fp, train_tn, train_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = best_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/test_imdb_reviews.csv')\n",
    "X_test = df['review']\n",
    "y_test = df['sentiment']\n",
    "\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 2636\n",
      "True Negatives: 1371\n",
      "False Positives: 431\n",
      "False Negatives: 192\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == y_pred[i]:\n",
    "        if y_test[i] == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    else:\n",
    "        if y_test[i] == 1:\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "print('True Positives:', tp)\n",
    "print('True Negatives:', tn)\n",
    "print('False Positives:', fp)\n",
    "print('False Negatives:', fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8654427645788337\n",
      "Precision:  0.859471796543854\n",
      "Recall:  0.9321074964639321\n",
      "F1:  0.8943172179813402\n",
      "AUC:  0.846464403059935\n",
      "4630\n"
     ]
    }
   ],
   "source": [
    "def auc(tp, fp, tn, fn):\n",
    "    return (tp / (tp + fn) + tn / (tn + fp)) / 2\n",
    "\n",
    "acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1: \", f1)\n",
    "print(\"AUC: \", auc(tp, fp, tn, fn))\n",
    "print(tp + tn + fp + fn)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
